{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dimensionality reduction Autoencoder"
      ],
      "metadata": {
        "id": "8HUZPnuE7Cao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pW8ryXgGEWa"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Load required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import tensorflow.keras.optimizers as optimizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#%%\n",
        "# Declaring file names\n",
        "# Path to input file\n",
        "ip_training_file = \"mirna_prot.csv\"\n",
        "# Path to output file\n",
        "op_file = \"AE_reduced_mirna_prot.csv\"\n",
        "# Save checkpoints while training\n",
        "checkpoint_filepath = \"AE_reduced_dataset_1_{epoch:02d}_{val_loss:.2f}.hdf5\"\n",
        "# Path to save AE loss image\n",
        "fig_path =  \"AE_reduced_dataset_1_loss.png\"\n",
        "\n",
        "#%%\n",
        "# Data preprocessing\n",
        "print(\"Reading data \\n\")\n",
        "training_data = pd.read_csv(ip_training_file, index_col=0)\n",
        "training_data.shape\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, train_ground,valid_ground = train_test_split(training_data, training_data, test_size = 0.1, shuffle = True, random_state = 122131)\n",
        "X_train.shape\n",
        "X_test.shape\n",
        "\n",
        "#%%\n",
        "# Function to save weights when there is reduction in validation loss\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath = checkpoint_filepath, save_weights_only = True, verbose = 1, monitor = 'val_loss', mode = 'min', save_best_only = True)\n",
        "\n",
        "# Early stopping function i.e stop training the model if the validation loss remains same for five subsequent epochs\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
        "\n",
        "# Declare number of layers and nodes in each layer of AE\n",
        "# Size of input data i.e dimension of input data, required to declare number of nodes in first and last layer of AE\n",
        "ip_dim_size = X_train.shape[1]\n",
        "# Size of encoded representations\n",
        "encoding_dim_1 = 500\n",
        "encoding_dim_2 = 100\n",
        "encoding_dim_3 = 25\n",
        "\n",
        "\n",
        "\n",
        "# Declare structure of AE\n",
        "# Encoder section\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "ip_dim_shape = Input(shape = (ip_dim_size,), name=\"input_layer\")\n",
        "x = Dense(encoding_dim_1, activation='relu', kernel_initializer = 'uniform', name = 'encoder_layer_1')(ip_dim_shape)\n",
        "x = Dense(encoding_dim_2, activation='relu', kernel_initializer = 'uniform', name = 'encoder_layer_2')(x)\n",
        "#x = Dense(encoding_dim_3, activation='relu', kernel_initializer = 'uniform', name = 'encoder_layer_3')(x)\n",
        "#x = Dense(encoding_dim_4, activation='relu', kernel_initializer = 'uniform', name = 'encoder_layer_4')(x)\n",
        "encoded = Dense(encoding_dim_2, activation='relu', kernel_initializer = 'uniform', name=\"bottleneck_layer\")(x)\n",
        "\n",
        "# Decoder section\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "#y = Dense(encoding_dim_4, activation='relu', kernel_initializer = 'uniform', name=\"decoder_layer_4\")(encoded)\n",
        "#y = Dense(encoding_dim_3, activation='relu', kernel_initializer = 'uniform', name=\"decoder_layer_3\")(encoded)\n",
        "y = Dense(encoding_dim_2, activation='relu', kernel_initializer = 'uniform', name=\"decoder_layer_2\")(encoded)\n",
        "y = Dense(encoding_dim_1, activation='relu', kernel_initializer = 'uniform', name=\"decoder_layer_1\")(y)\n",
        "decoded = Dense(ip_dim_size, activation='relu', kernel_initializer = 'uniform', name=\"op_layer\")(y)\n",
        "\n",
        "# This model maps an input to its reconstruction\n",
        "autoencoder = Model(inputs = ip_dim_shape, outputs = decoded)\n",
        "autoencoder.summary()\n",
        "\n",
        "# Hyper-paramters to train the model\n",
        "learning_rate = 0.001 # Vary or put in loop\n",
        "batch_size = 24 # Vary depending on the memory of GPU\n",
        "epochs = 200\n",
        "# Initialize required optimizer with learning rate\n",
        "opt_adam = optimizer.Adam(learning_rate = learning_rate)\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer = opt_adam, loss = 'mean_squared_error')\n",
        "\n",
        "# Train/fit the model\n",
        "estimator = autoencoder.fit(X_train, X_train, # input and output data\n",
        "                epochs = epochs,\n",
        "                batch_size = batch_size,\n",
        "                shuffle = True,\n",
        "                validation_data = (X_test, X_test), # Validation data\n",
        "                callbacks = [early_stopping_callback, model_checkpoint_callback])\n",
        "\n",
        "#%%\n",
        "# Get reduced dimensional representation\n",
        "# Get the encoder section of AE model\n",
        "encoder = Model(inputs = ip_dim_shape, outputs = encoded)\n",
        "# Get reduced dimension data for input data\n",
        "bottleneck_representation = encoder.predict(training_data)\n",
        "bottleneck_representation.shape\n",
        "# Convert result to dataframe - required to write to csv file\n",
        "bottleneck_representation_df = pd.DataFrame(data = bottleneck_representation, index = training_data.index)\n",
        "print(bottleneck_representation_df.shape)\n",
        "print(bottleneck_representation_df.iloc[0:5, 0:5])\n",
        "# Write result to csv file\n",
        "bottleneck_representation_df.to_csv(op_file, index = True)\n",
        "\n",
        "#%%\n",
        "# Obtain loss plot\n",
        "print(\"Training Loss: \", estimator.history['loss'][-1])\n",
        "print(\"Validation Loss: \", estimator.history['val_loss'][-1])\n",
        "plt.plot(estimator.history['loss'])\n",
        "plt.plot(estimator.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train','Validation'], loc = 'upper right')\n",
        "plt.savefig(fig_path)\n",
        "plt.close()"
      ]
    }
  ]
}